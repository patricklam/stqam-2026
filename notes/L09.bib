@Article{burkhardt67:_gener,
  author =       {W. H. Burkhardt},
  title =        {Generating test programs from syntax},
  journal =      {Computing},
  year =         1967,
  volume =    2,
  number =    1,
  pages =     {53--73},
  month =     {March}}



@inproceedings{yang11:_findin_c,
author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
title = {Finding and understanding bugs in {C} compilers},
year = 2011,
isbn = 9781450306638,
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993532},
doi = {10.1145/1993498.1993532},
abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {283--294},
numpages = 12,
keywords = {random testing, random program generation, compiler testing, compiler defect, automated testing},
location = {San Jose, California, USA},
series = {PLDI '11}
}


@inproceedings{le14:_compil,
author = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
title = {Compiler validation via equivalence modulo inputs},
year = 2014,
isbn = 9781450327848,
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594334},
doi = {10.1145/2594291.2594334},
abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {216--226},
numpages = 11,
keywords = {automated testing, compiler testing, equivalent program variants, miscompilation},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@inproceedings{holler12:_fuzzin,
author = {Holler, Christian and Herzig, Kim and Zeller, Andreas},
title = {Fuzzing with code fragments},
year = 2012,
publisher = {USENIX Association},
address = {USA},
abstract = {Fuzz testing is an automated technique providing random data as input to a software system in the hope to expose a vulnerability. In order to be effective, the fuzzed input must be common enough to pass elementary consistency checks; a JavaScript interpreter, for instance, would only accept a semantically valid program. On the other hand, the fuzzed input must be uncommon enough to trigger exceptional behavior, such as a crash of the interpreter. The LangFuzz approach resolves this conflict by using a grammar to randomly generate valid programs; the code fragments, however, partially stem from programs known to have caused invalid behavior before. LangFuzz is an effective tool for security testing: Applied on the Mozilla JavaScript interpreter, it discovered a total of 105 new severe vulnerabilities within three months of operation (and thus became one of the top security bug bounty collectors within this period); applied on the PHP interpreter, it discovered 18 new defects causing crashes.},
booktitle = {Proceedings of the 21st USENIX Conference on Security Symposium},
pages = 38,
numpages = 1,
location = {Bellevue, WA},
series = {Security'12}
}

@article{bettscheider25:_infer_input_gramm_code_symbol_parsin,
author = {Bettscheider, Leon and Zeller, Andreas},
title = {Inferring Input Grammars from Code with Symbolic Parsing},
year = 2025,
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3776743},
doi = {10.1145/3776743},
abstract = {Generating effective test inputs for a software system requires that these inputs be valid, as they will otherwise be rejected without reaching actual functionality. In the absence of a specification for the input language, common test generation techniques rely on sample inputs, which are abstracted into matching grammars and/or evolved guided by test coverage. However, if sample inputs miss features of the input language, the chances of generating these features randomly are slim.In this work, we present the first technique for symbolically and automatically mining input grammars from the code of recursive descent parsers. So far, the complexity of parsers has made such a symbolic analysis challenging to impossible. Our realization of the symbolic parsing technique overcomes these challenges by (1) associating each parser function parse_ELEM() with a nonterminal <ELEM>; (2) limiting recursive calls and loop iterations, such that a symbolic analysis of parse_ELEM() needs to consider only a finite number of paths; and (3) for each path, create an expansion alternative for <ELEM>. Being purely static, symbolic parsing does not require seed inputs; as it mitigates path explosion, it scales to complex parsers.Our evaluation promises symbolic parsing to be highly accurate. Applied on parsers for complex languages such as TINY-C or JSON, our STALAGMITE implementation extracts grammars with an accuracy of 99â€“100\%, widely improving over the state of the art despite requiring only the program code and no input samples. The resulting grammars cover the entire input space, allowing for comprehensive and effective test generation, reverse engineering, and documentation.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {Input grammars, symbolic analysis, test generation, fuzzing}
}

@inproceedings{hodovan18:_gramm,
author = {Hodov\'{a}n, Ren\'{a}ta and Kiss, \'{A}kos and Gyim\'{o}thy, Tibor},
title = {Grammarinator: a grammar-based open source fuzzer},
year = 2018,
isbn = 9781450360531,
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278193},
doi = {10.1145/3278186.3278193},
abstract = {Fuzzing, or random testing, is an increasingly popular testing technique. The power of the approach lies in its ability to generate a large number of useful test cases without consuming expensive manpower. Furthermore, because of the randomness, it can often produce unusual cases that would be beyond the awareness of a human tester. In this paper, we present Grammarinator, a general purpose test generator tool that is able to utilize existing parser grammars as models. Since the model can act both as a parser and as a generator, the tool can provide the capabilities of both generation and mutation-based fuzzers. The presented tool is actively used to test various JavaScript engines and has found more than 100 unique issues.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {45--48},
numpages = 4,
keywords = {security, random testing, grammars, fuzzing},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
}

@ARTICLE{luke00:_two,
  author={Luke, Sean},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Two fast tree-creation algorithms for genetic programming}, 
  year=2000,
  volume=4,
  number=3,
  pages={274--283},
  keywords={Genetic programming;Genetic mutations;Algorithm design and analysis;Functional programming;Optimization methods;Computer science;Size control;Tree data structures;Time factors},
  doi={10.1109/4235.873237}}
