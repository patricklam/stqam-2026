\documentclass[11pt]{article}
\usepackage{enumitem}
\usepackage{listings}
\usepackage[listings]{tcolorbox}
\usepackage{tikz}
\usepackage{url}
\usepackage{inconsolata}
\usepackage{pythonhighlight}

%\usepackage{algorithm2e}
\usetikzlibrary{arrows,automata,shapes}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{bt} = [rectangle, draw, fill=blue!20, 
    text width=4em, text centered, rounded corners, minimum height=2em]

\lstset{ %
language=Java,
basicstyle=\ttfamily,commentstyle=\scriptsize\itshape,showstringspaces=false,breaklines=true,numbers=left}
\newtcbinputlisting{\codelisting}[3][]{
    extrude left by=1em,
    extrude right by=2em,
    listing file={#3},
    fonttitle=\bfseries,
    listing options={basicstyle=\ttfamily\footnotesize,numbers=left,language=Java,#1},
    listing only,
    hbox,
}
\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, 
do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]''
}


\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf Software Testing, Quality Assurance and Maintenance } \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{#4}{Lecture #1}}
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
\newcommand{\brac}[1]{\texttt{\textless #1\textgreater}}

\begin{document}

\lecture{9 --- February 2, 2026}{Winter 2026}{Patrick Lam}{version 1}

We said that there was mutation-based fuzzing and grammar-based fuzzing.
Let's look at grammar-based fuzzing now.

\section*{Grammar-based fuzzing}
Regular expressions and context-free grammars are core CS 241
material, for those of you who are Software Engineering students. In
CS 241, you used regexps and CFGs to parse input.

\paragraph{Examples.}
Here is a Perl-syntax regular expression for (some) Visa credit card numbers:

\begin{verbatim}
                        ^4[0-9]{12}(?:[0-9]{3})?$
\end{verbatim}

It says that a Visa number starts (\verb+^+) with a 4, then contains
12 digits between 0 and 9, optionally another 3 more digits, and ends.
(This regexp is old; today, Visa numbers can be 19 digits).

You can use this regular expression to check that a given number is a
valid Visa number. Or---especially useful for the purpose of this
course---you can use it to generate numbers of the right shape.
You can imagine how to do that, and we'll write out implementations to
do so today.

It turns out, though, that 90\% of numbers that you generate from this
regular expression are definitely \emph{not} valid Visa numbers,
because credit card numbers have a checksum as the last digit,
computed using the Luhn algorithm. This algorithm can't be specified using
either regular expressions or context-free grammars. We'll get back to this point.

Moving along, we also have context-free grammars. The standard example is expressions.
\begin{verbatim}
<start>   ::= <expr>
<expr>    ::= <term> + <expr> | <term> - <expr> | <term>
<term>    ::= <term> * <factor> | <term> / <factor> | <factor>
<factor>  ::= +<factor> | -<factor> | (<expr>) | <integer> | <integer>.<integer>
<integer> ::= <digit><integer> | <digit>
<digit>   ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
\end{verbatim}
For our purposes, CFGs are the same as regular expressions, in the sense
that we can recognize strings in the language, and we can generate strings
from the language. The difference in expressiveness is not that important to us.

Thus: when programs' input formats are determined by grammars, then it is
possible to use the grammar backwards to generate inputs. These test
inputs are going to be more interesting than totally random sequences
of characters---if properly designed, they can test system behaviour
beyond just superficial input validation (as we saw in Lecture 7). In
particular, we aim to create grammars that specify (a superset of) the
legal set of inputs to our system under test.

Although character-based input formats are easiest, it's also possible
to use grammars for configurations, APIs, GUIs, etc.


\subsection*{Generating from Context-Free Grammars (CFGs)}
We'll continue by talking about grammars, as seen in
\url{https://www.fuzzingbook.org/html/Grammars.html}.

Recall that regular expressions can specify regular languages,
as can be recognized by a finite state machine (FSM). FSMs can't
count---for instance, specifically, they can't match parentheses.

Context-free grammars are more powerful than regular expressions, and
they can count. Even so, as I wrote above, some restrictions can't be
expressed in context-free languages/recognized by context-free
grammars, like the Luhn algorithm checksum requirement.

Here's another grammar, expressing the set of two-digit numbers.
\begin{verbatim}
<start> ::= <digit><digit>
<digit> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
\end{verbatim}
(This could also be expressed as a regular expression.)

We are aiming to generate inputs from a grammar. Let's do that
for this example.
\begin{enumerate}[noitemsep]
\item We start with the \brac{start} production and
see that we have two instances of the \brac{digit} nonterminal.
\item For each \brac{digit}, we visit its production, which says that
there are 10 valid alternatives for a digit, each of them a
distinct terminal.
\item To generate an input, we randomly choose one alternative for
  each digit, say ``1'' and ``7''. Concatenating our choices, we get a
  result like ``17''.
\end{enumerate}
This is grammar fuzzing. We can also carry out grammar fuzzing for the expressions example.
\begin{enumerate}[noitemsep]
\item Once again, start at \brac{start}.
\item There is one alternative, which is \brac{expr}. Visit \brac{expr}.
\item \brac{expr} can be three things. Randomly choose \brac{term} \texttt{+} \brac{expr}. Visit \brac{term},
  with \texttt{+} and \brac{expr} still to generate.
\item There are also 3 alternatives for \brac{term}. Randomly choose \brac{factor}.
\item Of the 5 alternatives for \brac{factor}, choose \brac{integer}.
\item There are 2 alternatives at \brac{integer}. Choose \brac{digit}.
\item Randomly choose the terminal \texttt{4} and generate it. We've bottomed out and continue with the remaining
  bits of the \texttt{expr}, which are \texttt{+} and \texttt{expr}.
\item \ldots
\end{enumerate}
One possible thing we might generate, if we finish this example, is \texttt{4 + 22 * 5.3}.
We can randomly generate lots of expressions from this grammar, and they are all valid expressions.

\subsection*{Representing Grammars in Python}
In Java we would set up a class hierarchy, and in Python we could, but perhaps it's more idiomatic to just use
Python's data structures to represent grammars, with conventions for meaning. We can represent a grammar
in Python as a mapping from the alternative's left-hand side (LHS) to its right-hand side (RHS), e.g.
\begin{python}
DIGIT_GRAMMAR = {
    "<start>":
        ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
}
\end{python}
Nonterminals are in \brac{brackets} and the rest of a string is taken as terminals.
This is a single-production grammar that says that the start symbol can resolve to one of the
10 alternatives, which are terminals representing the digits 0 through 9.

A type hint that works in Python 3.12 and up is:
\begin{python}
from typing import Dict, List
type Expansion = str
type Grammar = Dict[str, List[Expansion]]
\end{python}
(in older Pythons, omit the keyword \texttt{type}).

We can express the expression grammar above in our Python representation also:
\begin{python}
EXPR_GRAMMAR: Grammar = {
    "<start>":
        ["<expr>"],
    "<expr>":
        ["<term> + <expr>", "<term> - <expr>", "<term>"],
    "<term>":
        ["<factor> * <term>", "<factor> / <term>", "<factor>"],
    "<factor>":
        ["+<factor>", "-<factor>",
         "(<expr>)",
         "<integer>.<integer>", "<integer>"],
    "<integer>":
        ["<digit><integer>", "<digit>"],
    "<digit>":
        ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
  }
\end{python}
Since it's a mapping, we can use lookups and inclusion tests:
\begin{python}
>>> EXPR_GRAMMAR["<digit>"]
>>> "<integer>" in EXPR_GRAMMAR
\end{python}

Our grammars must always have start symbol \brac{START}:
\begin{python}
START_SYMBOL = "<start>"
\end{python}

Here's some helper functions for nonterminals, using a regular expression.
\begin{python}
import re
RE_NONTERMINAL = re.compile(r'(<[^<> ]*>)')
def nonterminals(expansion):
    # In later chapters, we allow expansions to be tuples,
    # with the expansion being the first element
    if isinstance(expansion, tuple):
        expansion = expansion[0]

    return RE_NONTERMINAL.findall(expansion)

def is_nonterminal(s):
    return RE_NONTERMINAL.match(s)
\end{python}
There are more examples in the Fuzzing Book, but we can do this:
\begin{python}
>>> assert nonterminals("<term> * <factor>") == ["<term>", "<factor>"]
>>> assert is_nonterminal("<symbol-1>")
\end{python}

\subsection*{A Simple Grammar Fuzzer}
We have set up enough infrastructure to write a simple string-replacement-based fuzzer now.
\begin{python}
import random
class ExpansionError(Exception):
  pass
def simple_grammar_fuzzer(grammar: Grammar, 
                          start_symbol: str = START_SYMBOL,
                          max_nonterminals: int = 10,
                          max_expansion_trials: int = 100,
                          log: bool = False) -> str:
    """Produce a string from `grammar`.
       `start_symbol`: use a start symbol other than `<start>` (default).
       `max_nonterminals`: the maximum number of nonterminals 
         still left for expansion
       `max_expansion_trials`: maximum # of attempts to produce a string
       `log`: print expansion progress if True"""

    term = start_symbol
    expansion_trials = 0

    while len(nonterminals(term)) > 0:
        symbol_to_expand = random.choice(nonterminals(term))
        expansions = grammar[symbol_to_expand]
        expansion = random.choice(expansions)
        if isinstance(expansion, tuple):
            expansion = expansion[0]

        new_term = term.replace(symbol_to_expand, expansion, 1)

        if len(nonterminals(new_term)) < max_nonterminals:
            term = new_term
            if log:
                print("%-40s" % (symbol_to_expand + " -> " + expansion), term)
            expansion_trials = 0
        else:
            expansion_trials += 1
            if expansion_trials >= max_expansion_trials:
                raise ExpansionError("Cannot expand " + repr(term))

    return term
\end{python}
You can find this in \texttt{code/L09/simple\_grammar\_fuzzer.py} and you can run this fuzzer
with the invocation:
\begin{python}
>>> simple_grammar_fuzzer(grammar=EXPR_GRAMMAR, max_nonterminals=3, log=True)
\end{python}
This function takes a string, finds a nonterminal (using the brackets),
picks an alternative for that nonterminal, and replaces the nonterminal
with one of the permissible RHSs. It is all string replacement. One might
do this by tree manipulations instead, which would be more effective,
but less quick-and-dirty. There are some hard limits on expansions to guarantee
fast-enough termination.

\subsection*{Other Grammar Examples}
The \emph{Fuzzing Book} talks about railroad diagrams for visualizing grammars,
and provides code to use libraries to do that. We don't, but we may include some
pictures from there as needed.

We will, however, provide more grammars. Here's the grammar accepted by the
\texttt{cgi\_decode()} function we saw earlier.

\begin{python}
CGI_GRAMMAR: Grammar = {
    "<start>":
        ["<string>"],
    "<string>":
        ["<letter>", "<letter><string>"],
    "<letter>":
        ["<plus>", "<percent>", "<other>"],
    "<plus>":
        ["+"],
    "<percent>":
        ["%<hexdigit><hexdigit>"],
    "<hexdigit>":
        ["0", "1", "2", "3", "4", "5", "6", "7",
            "8", "9", "a", "b", "c", "d", "e", "f"],
    "<other>":  # Actually, could be _all_ letters
        ["0", "1", "2", "3", "4", "5", "a", "b", "c", "d", "e", "-", "_"],
}
\end{python}
and we can produce CGI strings:
\begin{python}
for i in range(10):
    print(simple_grammar_fuzzer(grammar=CGI_GRAMMAR, max_nonterminals=10))
\end{python}
The hit rate for valid CGI strings is much higher with grammar fuzzing than
for basic fuzzing and for mutation-based fuzzing.

Here's another example:
\begin{python}
URL_GRAMMAR: Grammar = {
    "<start>":
        ["<url>"],
    "<url>":
        ["<scheme>://<authority><path><query>"],
    "<scheme>":
        ["http", "https", "ftp", "ftps"],
    "<authority>":
        ["<host>", "<host>:<port>", "<userinfo>@<host>", "<userinfo>@<host>:<port>"],
    "<host>":  # Just a few
        ["patricklam.ca", "www.google.com", "fuzzingbook.com"],
    "<port>":
        ["80", "8080", "<nat>"],
    "<nat>":
        ["<digit>", "<digit><digit>"],
    "<digit>":
        ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
    "<userinfo>":  # Just one
        ["user:password"],
    "<path>":  # Just a few
        ["", "/", "/<id>"],
    "<id>":  # Just a few
        ["abc", "def", "x<digit><digit>"],
    "<query>":
        ["", "?<params>"],
    "<params>":
        ["<param>", "<param>&<params>"],
    "<param>":  # Just a few
        ["<id>=<id>", "<id>=<nat>"],
}
\end{python}
and we can push the button and generate URLs.
\begin{python}
for i in range(10):
    print(simple_grammar_fuzzer(grammar=URL_GRAMMAR, max_nonterminals=10))
\end{python}
It's a narrow subset of the entire space of valid URLs, but we get better
coverage of our chosen subset.

The \emph{Fuzzing Book} also provides an example for generating mad-lib style
book titles using grammars, but we'll omit that.

\paragraph{Validity of generated inputs.} To reiterate what we've said before:
if you have a grammar, you can easily generate strings belonging to the grammar.
Inputs generated from a grammar will always satisfy the grammar. However, they
may fail to be valid inputs for a program if there are some additional
constraints for inputs to be valid for that program. A port number
needing to be between 1024 and 2048 is hard. At least as hard is a
checksum requirement, as with credit card numbers.

It's possible to attach constraints to grammars to make sure one generates
more valid inputs. (Other, unrelated, things that one can do are to
eliminate redundancy and to weight some alternatives more heavily than others.)

\subsection*{Grammars as Mutation Seeds}
So far, we've generated only syntactically valid inputs from grammars,
which is helpful. But also, part of the point of fuzzing is to generate \emph{invalid}
inputs. Mutation can help with that, so back to mutation.

Our mutation fuzzer starts with a number of seeds and then mutates
them.  Grammar-based generation can first create valid seeds, and then the
mutation fuzzer can work from those. This can give a mix of valid and (slightly) invalid inputs,
which helps exercise the parser in ways that are difficult for humans to generate.

\begin{python}
number_of_seeds = 10
seeds = [
    simple_grammar_fuzzer(
        grammar=URL_GRAMMAR,
        max_nonterminals=10) for i in range(number_of_seeds)]
seeds
m = MutationFuzzer(seeds)
[m.fuzz() for i in range(20)]
\end{python}
If we run this, we can see that we start with 10 inputs generated by the grammars,
and then we have a mix of valid and invalid inputs that are fuzzed. One could
run a URL validator to see how many of them are actually valid, and also check to
see if they would parse according to the grammar (though we do not have infrastructure
for that).

\subsection*{Additional stuff}
The \emph{Fuzzing Book} has a bunch of grammar utilities which I won't define; I'll mention
them when they come up. I'm not sure about if you've seen EBNF, but it's a bunch of shortcuts
for grammars:
\begin{itemize}[noitemsep]
\item \brac{symbol}? means that \brac{symbol} can occur 0 or 1 times;
\item \brac{symbol}+ means that \brac{symbol} can occur 1 or more times;
\item \brac{symbol}* means that \brac{symbol} can occur 0 or more times;
\item parentheses can be used with these shortcuts, e.g. (\brac{s1}\brac{s2})+
\end{itemize}

So, instead of
\begin{python}
  "<identifier>": ["<idchar>", "<identifier><idchar>"],
\end{python}
we can just write
\begin{python}
  "<identifier>": ["<idchar>+"],
\end{python}
The \emph{Fuzzing Book} has a function \texttt{convert\_ebnf\_grammar()} that translates EBNF to BNF. You can
find these in \texttt{code/L09/ebnf.py}.

The code so far has also allowed expansions to be a pair, like
\begin{python}
"<expr>":
        [("<term> + <expr>", opts(min_depth=10)),
         ("<term> - <expr>", opts(max_depth=2)),
          "<term>"]
\end{python}
and there are some helper functions \texttt{opts()}, \texttt{exp\_string()}, \texttt{exp\_opt()}, \texttt{exp\_opts()},
\texttt{set\_opts()}. These live in \texttt{code/L09/opts.py}.

Finally, there is a utility function \texttt{trim\_grammar()} and a validation function \texttt{is\_valid\_grammar()}.
\texttt{trim\_grammar()} returns a semantically-equivalent grammar to what you passed in, without
unneeded expansions, while \texttt{is\_valid\_grammar()} does semantic checks on a grammar.

\subsection{Applications of Grammars in Testing}
The \emph{Fuzzing Book} lists some projects using grammar-based fuzzing, principally for compiler testing and also web browsers.
The earliest citation they have is for work by Burkhardt in 1967~\cite{burkhardt67:_gener} which claims to automatically generate
programs from grammars.

\paragraph{CSmith.} At some level, Csmith~\cite{yang11:_findin_c} uses grammar-based fuzzing to generate C programs,
finding over 400 previously unknown compiler bugs. But there is much more to it: they go to a lot of effort to make sure
that Csmith doesn't generate programs with undefined behaviour, which is well beyond what a grammar can guarantee.
There is also the notion of a pseudo-oracle here: one doesn't know what the right output is, so they compare
GCC and LLVM outputs.

\paragraph{EMI.} Still in the C compiler domain, but using a different technique, is Equivalence Modulo Inputs~\cite{le14:_compil}.
Here, the idea is to fuzz dead code (as observed by measuring
coverage) in valid C programs and observe compilers mis-compiling the
fuzzed programs. I wouldn't really put this in the grammars section,
but the \emph{Fuzzing Book} did.

\paragraph{LangFuzz.} By some of the authors of the \emph{Fuzzing Book}, the LangFuzz tool~\cite{holler12:_fuzzin} performs grammar-based fuzzing on
test suites that contained past bugs. There are still new bugs to be found in the vicinity of old bugs. They collected
a bunch of bug bounties; almost all of the bugs they found were memory safety bugs (so, implicit oracles).

(Tangentially, I also noticed that Andreas Zeller, one of the \emph{Fuzzing Book} authors,
recently published a paper~\cite{bettscheider25:_infer_input_gramm_code_symbol_parsin} where
they inferred the input grammar from recursive descent parser code.)

\paragraph{Grammarinator.} Seems to be similar to LangFuzz, but it's an open-source grammar fuzzer\footnote{\url{https://github.com/renatahodovan/grammarinator}} written in Python~\cite{hodovan18:_gramm}.

\paragraph{Domato.} Not just compilers: Domato~\footnote{\url{https://github.com/googleprojectzero/domato}} fuzzes 
Document Object Model input. Specifically, it generates fuzzed versions of HTML, CSS, and JavaScript files, using
grammars but starting from scratch. More about this project: \url{https://projectzero.google/2017/09/the-great-dom-fuzz-off-of-2017.html}.

\section*{Efficient Grammar Fuzzing}
Next up: some tricks to generate from grammars more efficiently, from \url{https://www.fuzzingbook.org/html/GrammarFuzzer.html}.

I found the design choice in \texttt{simple\_grammar\_fuzzer} to manipulate strings directly to be\ldots interesting. I would
have thought that one would use a tree, though there is more faffing with data structures. And indeed, we are now going to use trees,
and introduce various controls that we can use to control what trees get generated.

We had talked about a grammar for expressions earlier today. In \texttt{code/L09/ebnf.py} you'll find an EBNF grammar
for expressions. We can convert it to BNF so that we can work with it.

\begin{python}
expr_grammar = convert_ebnf_grammar(EXPR_EBNF_GRAMMAR)
\end{python}

Because we're talking about grammar fuzzing, let's try to fuzz this grammar. I'll anticipate timeouts. You can find this
in \texttt{code/L09/simple\_grammar\_fuzzer\_problem.py}.
\begin{python}
with ExpectTimeout(1):
  simple_grammar_fuzzer(grammar=expr_grammar, max_nonterminals=3)
\end{python}
This does indeed raise a \texttt{TimeoutError}.

You can look at the grammar, and it's a good exercise to do so. But if
you call \texttt{simple\_grammar\_fuzzer} with \texttt{log=True} you
can see the expansions, and the proximate reason why it times out
should become abundantly clear: too many parens. But why?

The rule for \brac{factor} has RHS:
\begin{python}
['<sign-1><factor>', '(<expr>)', '<integer><symbol-1>']
\end{python}
and our simple fuzzer throws away generated strings with more than the
maximum allowed number of non-terminals (3 in our case).  So the only
allowed string to be generated for \brac{factor} is
\texttt{'(\brac{expr})'}: the other alternatives all add a
non-terminal, and when at the limit, the fuzzer won't do that.

Any grammar-based approach can create arbitrarily long strings. The problem
with the approach as we have implemented it, though, is that the hard limit on
non-terminals prevents it from generating some strings that we want.

In general, the single \texttt{max\_nonterminals} control that \texttt{simple\_grammar\_fuzzer} provides us is insufficient.
And, manipulating strings is less efficient than manipulating trees: the fuzzer has to do a search through the string
for non-terminals.

The \emph{Fuzzing Book} continues with a graph showing that the time required to generate an output grows quadratically
with the output length, and also that very many outputs are tens of thousands of characters long.

We want an efficient grammar fuzzer, which doesn't get stuck on bad alternatives.

\subsection*{Derivation Trees}
The obvious choice for representing derivations is \emph{derivation
trees}. The nodes in the trees can be terminals or nonterminals;
nonterminals may have children corresponding to a derivation, as we'll
see.  The \emph{Fuzzing Book} says that they also known as concrete
syntax trees or parse trees, though my definition of a concrete syntax
tree is one where leaves are terminals.

Let's look at an example of deriving a string, again using the arithmetic expression grammar. We start with the \brac{start} node.
\begin{center}
\begin{tikzpicture}
  \node[bt] (s) {\brac{start}};
\end{tikzpicture}
\end{center}
In general, we search the tree for a nonterminal $S$ that does not
have children and hence is a candidate for expansion.  In this case,
there is only \brac{start}, and its only alternative is \brac{expr},
so we expand the tree by adding \brac{expr} as a child of
\brac{start}.
\begin{center}
\begin{tikzpicture}[node distance=1.3cm]
  \node[bt] (s)               {\brac{start}};
  \node[bt] (e0) [below of=s] {\brac{expr}};

  \path[->] (s) edge node {} (e0);
\end{tikzpicture}
\end{center}
This tree can be unparsed back to the string ``\brac{expr}'' by collecting the leaves of the tree.

We can continue the expansion process. Nonterminal \brac{expr} can expand to $\brac{term} + \brac{expr}$, yielding the tree:
\begin{center}
\begin{tikzpicture}[node distance=1.3cm]
  \node[bt] (s)               {\brac{start}};
  \node[bt] (e0) [below of=s] {\brac{expr}};
  \node[bt,text width=1cm] (+) [below of=e0] {+};
  \node[bt] (t) [left of=+,xshift=-1cm] {\brac{term}};
  \node[bt] (e1) [right of=+,xshift=1cm] {\brac{expr}};

  \path[->] (s) edge node {} (e0);
  \path[->] (e0) edge node {} (t);
  \path[->] (e0) edge node {} (+);
  \path[->] (e0) edge node {} (e1);
\end{tikzpicture}
\end{center}
Here we have one terminal, $+$, as well as two more nonterminals.

\newpage
We can further continue the expansion process until we get to terminals.
\begin{center}
\begin{tikzpicture}[node distance=1.3cm]
  \node[bt] (s)               {\brac{start}};
  \node[bt] (e0) [below of=s] {\brac{expr}};
  \node[bt,text width=1cm,fill=green!20] (+) [below of=e0] {+};
  \node[bt] (t0) [left of=+,xshift=-1cm] {\brac{term}};
  \node[bt,text width=2cm] (f0) [below of=t0] {\brac{factor}};
  \node[bt,text width=2cm] (i0) [below of=f0] {\brac{integer}};
  \node[bt] (d0) [below of=i0] {\brac{digit}};
  \node[bt,fill=green!20] (cd0) [below of=d0] {\brac{2}};
  \node[bt] (e1) [right of=+,xshift=1cm] {\brac{expr}};
  \node[bt] (t1) [below of=e1] {\brac{term}};
  \node[bt,text width=2cm] (f1) [below of=t1] {\brac{factor}};
  \node[bt,text width=2cm] (i1) [below of=f1] {\brac{integer}};
  \node[bt] (d1) [below of=i1] {\brac{digit}};
  \node[bt,fill=green!20] (cd1) [below of=d1] {\brac{3}};

  \path[->] (s) edge node {} (e0);
  \path[->] (e0) edge node {} (t0);
  \path[->] (t0) edge node {} (f0);
  \path[->] (f0) edge node {} (i0);
  \path[->] (i0) edge node {} (d0);
  \path[->] (d0) edge node {} (cd0);
  \path[->] (e0) edge node {} (+);
  \path[->] (e0) edge node {} (e1);
  \path[->] (e1) edge node {} (t1);
  \path[->] (t1) edge node {} (f1);
  \path[->] (f1) edge node {} (i1);
  \path[->] (i1) edge node {} (d1);
  \path[->] (d1) edge node {} (cd1);
\end{tikzpicture}
\end{center}
A derivation tree can yield the strings that we've seen before. But it
also records the nonterminals that were expanded to yield a given
final string. And, being a tree, it doesn't require string searches
and replacements; it's far more efficient to manipulate the tree.

\subsection*{Derivation Trees in Python}
Here's the way we represent derivation trees in Python. A node is a pair
\begin{python}
  (SYMBOL_NAME, CHILDREN)
\end{python}
where \texttt{SYMBOL\_NAME} is a string representing the node (e.g.
"\brac{start}" or "+") and \texttt{CHILDREN} generally represents the children.
So the node also contains the subtree starting at that node.

We also allow \texttt{CHILDREN} to be \texttt{None}, which means that this node is
a nonterminal still to be expanded; or, \texttt{CHIDLREN} can be \texttt{[]}, meaning
that this node is a terminal. There is an invariant that \texttt{SYMBOL\_NAME} is \brac{bracketed}
iff the node is a nonterminal.

We can thus state a Python type:
\begin{python}
DerivationTree = Tuple[str, Optional[List[Any]]]
\end{python}
which could actually be a recursive type using \texttt{DerivationTree} instead of \texttt{Any},
but the \emph{Fuzzing Book} says the Python type checker can't handle that, which I've confirmed.

Here is an example derivation tree for \texttt{\brac{expr} + \brac{term}}.
\begin{python}
derivation_tree: DerivationTree = ("<start>",
                   [("<expr>",
                     [("<expr>", None),
                      (" + ", []),
                         ("<term>", None)]
                     )])
\end{python}

There is a lot of implementation talk next in the \emph{Fuzzing Book}. I'll skip most of it, but include it
in \texttt{code/L09/derivation\_tree.py} and \texttt{code/L09/grammar\_fuzzer.py}.

I will say that there is a \texttt{GrammarFuzzer} class, which will take the following parameters:
\begin{python}
class GrammarFuzzer(Fuzzer):
    """Produce strings from grammars efficiently, using derivation trees."""

    def __init__(self,
                 grammar: Grammar,
                 start_symbol: str = START_SYMBOL,
                 min_nonterminals: int = 0,
                 max_nonterminals: int = 10,
                 disp: bool = False,
                 log: Union[bool, int] = False) -> None:
        # ...
\end{python}
Right now, we are only going to use the grammar and start symbol. There are also diagnostics
in the form of \texttt{disp} and \texttt{log}. The constructor calls \texttt{check\_grammar()},
which ensures some validity properties:
\begin{python}
      def check_grammar(self) -> None:
        """Check the grammar passed"""
        assert self.start_symbol in self.grammar
        assert is_valid_grammar(
            self.grammar,
            start_symbol=self.start_symbol,
            supported_opts=self.supported_opts())
\end{python}
We didn't talk about \texttt{is\_valid\_grammar}, but it checks that all nonterminals are reachable
from either \brac{start} or the start symbol that it is given.

\paragraph{Tree manipulation: expanding a node.} One of the central operations that we need to be able to perform
on a derivation tree is to randomly expand a nonterminal into one of its alternatives; for instance, an \brac{expr}
can be an addition, subtraction, or just a term, and we need to replace the \brac{expr} with, for instance,
$\brac{term} + \brac{expr}$.

Which alternative? We'll start by randomly choosing an alternative. But we'll want to use different strategies in the future.
\begin{python}
    def choose_node_expansion(self, node: DerivationTree,
                              children_alternatives: List[List[DerivationTree]]) -> int:
        """Return index of expansion in `children_alternatives` to be selected.
           'children_alternatives`: a list of possible children for `node`.
           Defaults to random. To be overloaded in subclasses."""
        return random.randrange(0, len(children_alternatives))
\end{python}

Despite moving to derivation trees, we still express our alternatives as strings. (Indeed, that's more user-friendly
than the alternative). We need to convert alternatives into something derivation-tree-friendly.
As an example, our grammar contains the alternative $\brac{term} + \brac{expr}$ for \brac{expr}.
We want to convert it to the list of nodes
\begin{python}
  [('<term>', None), (' + ', []), ('<expr>', None)]
\end{python}
and we do so using function \texttt{expansion\_to\_children}, which can be overridden:
\begin{python}
def expansion_to_children(expansion: Expansion) -> List[DerivationTree]:
    # print("Converting " + repr(expansion))
    # strings contains all substrings -- both terminals and nonterminals such
    # that ''.join(strings) == expansion

    expansion = exp_string(expansion)
    assert isinstance(expansion, str)

    if expansion == "":  # Special case: epsilon expansion
        return [("", [])]

    strings = re.split(RE_NONTERMINAL, expansion)
    return [(s, None) if is_nonterminal(s) else (s, [])
            for s in strings if len(s) > 0]
\end{python}
So, given a subtree with a non-expanded non-terminal at its root, we can expand this non-terminal into one of its
alternatives, per the grammar, and return the new tree. Recall that being non-expanded is represented by
having \texttt{children} be \texttt{None}.
\begin{python}
    def expand_node_randomly(self, node: DerivationTree) -> DerivationTree:
        """Choose a random expansion for `node` and return it"""
        (symbol, children) = node
        assert children is None

        if self.log:
            print("Expanding", all_terminals(node), "randomly")

        # Fetch & parse the possible expansions from grammar...
        expansions = self.grammar[symbol]
        children_alternatives: List[List[DerivationTree]] = [
            self.expansion_to_children(expansion) for expansion in expansions
        ]

        # ... and select a random expansion
        index = self.choose_node_expansion(node, children_alternatives)
        chosen_children = children_alternatives[index]

        # Process children (for subclasses) [do nothing for now]
        chosen_children = self.process_chosen_children(chosen_children,
                                                       expansions[index])

        # Return with new children
        return (symbol, chosen_children)
\end{python}
Randomly is one strategy, and for now our only strategy, though we will redefine \texttt{expand\_node} as needed
to use other strategies.
\begin{python}
    def expand_node(self, node: DerivationTree) -> DerivationTree:
        return self.expand_node_randomly(node)
\end{python}
Note that \texttt{expand\_node} does not modify the input \texttt{node} but indeed returns a fresh \texttt{DerivationTree},
which is consistent with functional programming style.

We can try out \texttt{expand\_node}:
\begin{python}
    f = GrammarFuzzer(EXPR_GRAMMAR, log=True)

    print("Before expand_node_randomly():")
    expr_tree = ("<integer>", None)
    print (display_tree(expr_tree))

    expr_tree = f.expand_node_randomly(expr_tree)
    print("After expand_node_randomly():")
\end{python}
which starts with the tree with unexpanded single node (\brac{integer}, \texttt{None}), and can finish with
either the tree
\begin{python}
  ('<integer>', [('<digit>', None), ('<integer>', None)])
\end{python}
or the tree
\begin{python}
  ('<integer>', [('<digit>', None)])
\end{python}
as randomly chosen.

\paragraph{Tree manipulation: expanding the tree.}
We don't just want to be able to expand the root of the tree. We need to be able to find some unexpanded
nonterminal in the tree and expand that. Method \texttt{expand\_tree\_once()} expands the root if it is
unexpanded. Otherwise, it looks for any (transitive) children which are unexpanded. We do not need
to look at this implementation, but \texttt{expand\_tree\_once} and \texttt{any\_possible\_expansions}
are in the repo.

Note that unlike \texttt{expand\_node\_randomly()}, this method \texttt{expand\_tree\_once()}
modifies the tree in-place, which is more efficient.

This method also hardcodes calls to the method \texttt{expand\_node()},
but it's Python, so you can, and we do, redefine that method several times to use different strategies; in the example below, it is
set to \texttt{expand\_node\_randomly()}.

For instance, we can apply \texttt{expand\_tree\_once()} twice:
\begin{python}
derivation_tree = ("<start>",
                   [("<expr>",
                     [("<expr>", None), (" + ", []), ("<term>", None)])])
print (derivation_tree)
f.expand_tree_once(derivation_tree)
f.expand_tree_once(derivation_tree)
print (derivation_tree)
\end{python}

\paragraph{Tree manipulation: closing the expansion.}
If we continue randomly choosing nonterminals, we will probably get
derivation trees that are bigger than we'd like.  I think that random
expansion has finite expected termination time if there is a path to
termination, but we would like smaller trees.

The insight that we'll apply, from \cite{luke00:_two}, is that, after inflating the
tree up to a size bound, we then choose expansions that increase the tree size least.
The \brac{factor} production, for instance, has alternative $\brac{integer}(.\brac{integer})?$,
which could be \brac{integer}, and \brac{integer} could be a single \brac{digit}.
Other alternatives are higher-cost.

We can compute the minimum cost by doing a tree traversal and summing the minima for each
production.
\begin{python}
    def symbol_cost(self, symbol: str, seen: Set[str] = set()) \
            -> Union[int, float]:
        expansions = self.grammar[symbol]
        return min(self.expansion_cost(e, seen | {symbol}) for e in expansions)

    def expansion_cost(self, expansion: Expansion,
                       seen: Set[str] = set()) -> Union[int, float]:
        symbols = nonterminals(expansion)
        if len(symbols) == 0:
            return 1  # no symbol

        if any(s in seen for s in symbols):
            return float('inf')

        # the value of a expansion is the sum of all expandable variables
        # inside + 1
        return sum(self.symbol_cost(s, seen) for s in symbols) + 1
\end{python}

At a symbol, the symbol cost is the minimum of its children (1 for terminals and
recursively calculated for nonterminals). Given an alternative (aka expansion),
its minimum expansion cost is 1 if there are no nonterminals in that alternative, or 1 plus the sum of the nonterminals otherwise.
But, this loops infinitely if there is a cycle involving a nonterminal, and so we use \texttt{float('inf')} in that case,
which works properly with the Python \texttt{min} function.

We can check that the minimum cost of expanding \brac{digit} is 1:
\begin{python}
f = GrammarFuzzer(EXPR_GRAMMAR)
assert f.symbol_cost("<digit>") == 1
\end{python}
while the minimum cost of expanding \brac{expr} is 5, through \brac{term}, \brac{factor}, \brac{integer}, \brac{digit}, and 1.
\begin{python}
assert f.symbol_cost("<expr>") == 5
\end{python}

Previously, we chose a node to expand randomly (and limited the number of nonterminals). We now parametrize
by choice algorithm, \texttt{choose}:
\begin{python}
    def expand_node_by_cost(self, node: DerivationTree, 
                            choose: Callable = min) -> DerivationTree:
\end{python}
I won't show the implementation, but I'll describe it. This method
collects (in a list) the minimum cost of all of the alternatives, asks
function \texttt{choose} to choose one of the costs in the list, then
goes back and collects all alternatives with that cost, and uses
\texttt{choose\_node\_expansion()} to choose one of those alternatives
(default: random). It calls a hook to post-process the chosen
alternative, and returns it.  This method is functional and does not
modify the node it is given. Again, you can find the code in
\texttt{code/L09/grammar\_fuzzer.py}.

We can use \texttt{min} as the choice algorithm:
\begin{python}
    def expand_node_min_cost(self, node: DerivationTree) -> DerivationTree:
        if self.log:
            print("Expanding", all_terminals(node), "at minimum cost")

        return self.expand_node_by_cost(node, min)
\end{python}
and we can use that to get a sequence of expansions:
\begin{python}
    f.expand_node = f.expand_node_min_cost
    while f.any_possible_expansions(derivation_tree):
        derivation_tree = f.expand_tree_once(derivation_tree)    
    print (derivation_tree)
    print (all_terminals(derivation_tree))
\end{python}
which might print out, for instance,
\begin{python}
('<start>', [('<expr>', [('<expr>', [('<term>', [('<factor>', [('<integer>', [('<digit>', [('4', [])])])])])]), (' + ', []), ('<term>', [('<factor>', [('<integer>', [('<digit>', [('0', [])])])]), (' * ', []), ('<term>', [('<factor>', [('<integer>', [('<digit>', [('9', [])])])])])])])])
4 + 0 * 9
\end{python}
The \emph{Fuzzing Book} points out that \texttt{expand\_node\_min\_cost()} does not increase the number of symbols, so
that all open expansions are closed. They showed more steps than I did. You can't see it from these notes, but you can
work through it and convince yourself that this is true, though being able to close all expansions is also a property
of the grammar, in addition to the choice of node.

\paragraph{Tree manipulation: node inflation.} We also want to have trees of at least a certain size, so that our
programs have something to work with. We can easily look for the max-cost expansion using our machinery as well.
\begin{python}
    def expand_node_min_cost(self, node: DerivationTree) -> DerivationTree:
        if self.log:
            print("Expanding", all_terminals(node), "at maximum cost")

        return self.expand_node_by_cost(node, max)
\end{python}

We can repeat a couple of times:
\begin{python}
    derivation_tree = ("<start>",
                   [("<expr>",
                     [("<expr>", None),
                      (" + ", []),
                         ("<term>", None)]
                     )])
    f = GrammarFuzzer(EXPR_GRAMMAR, log=True)
    f.expand_node = f.expand_node_max_cost

    # do this a number of times
    if f.any_possible_expansions(derivation_tree):
        derivation_tree = f.expand_tree_once(derivation_tree)
    print (derivation_tree)
    print (all_terminals(derivation_tree))
\end{python}

\paragraph{Max, random, min.} What we want is to combine all three types of tree expansion. Start by inflating the tree (max cost expansions), do
some random expansions, and then close the tree (min cost
expansions). The inflation step continues until there are at least
\texttt{min\_nonterminals} nonterminals (possibly set to 0), continue
randomly expanding until there are at least \texttt{max\_nonterminals} (or
maybe the tree just closes itself), and then, if the grammar is
properly designed, we get all terminals.

\begin{python}
    def expand_tree(self, tree: DerivationTree) -> DerivationTree:
        """Expand `tree` in a three-phase strategy until all expansions are complete."""
        self.log_tree(tree)
        tree = self.expand_tree_with_strategy(
            tree, self.expand_node_max_cost, self.min_nonterminals)
        tree = self.expand_tree_with_strategy(
            tree, self.expand_node_randomly, self.max_nonterminals)
        tree = self.expand_tree_with_strategy(
            tree, self.expand_node_min_cost)

        assert self.possible_expansions(tree) == 0
        return tree
\end{python}
We omit \texttt{expand\_tree\_with\_strategy}, but it repeatedly calls
the given strategy's \texttt{expand\_node} until the tree has at least
the given limit of possible expansions.

We can try it again on the same initial derivation tree as before.
\begin{python}
# derivation tree as before
f = GrammarFuzzer(
    EXPR_GRAMMAR,
    min_nonterminals=3,
    max_nonterminals=5,
    log=True)
f.expand_tree(derivation_tree)
\end{python}
What you'll probably see is one expansion at max cost, a few randoms, and then a sequence of min cost expansions.
I end up with the tree of terminals \verb/5 - 9 - 5 + 1 * 9/ on one run, though of course you can get others (that's
the whole point).

\paragraph{Just a fuzzer.} We can abstract everything and provide just the \texttt{fuzz()} interface.
\begin{python}
    def fuzz_tree(self) -> DerivationTree:
        """Produce a derivation tree from the grammar."""
        tree = self.init_tree()
        # print(tree)

        # Expand all nonterminals
        tree = self.expand_tree(tree)
        if self.log:
            print(repr(all_terminals(tree)))
        if self.disp:
            display(display_tree(tree))
        return tree

    def fuzz(self) -> str:
        """Produce a string from the grammar."""
        self.derivation_tree = self.fuzz_tree()
        return all_terminals(self.derivation_tree)  
\end{python}
and we can invoke it easily:
\begin{python}
    f = GrammarFuzzer(EXPR_GRAMMAR)
    print (f.fuzz())
\end{python}
There are sensible defaults for \texttt{min\_nonterminals} and \texttt{max\_nonterminals},
and we get a relatively large input from the fuzzer. You can also retrieve the derivation tree
by reading the \texttt{.derivation\_tree} property. One output you might get is:
\begin{python}
+-(-1 * 8 * 7 - 9 * 5 + 4) / -2 + 031.429 * 2 * +--(+(8 + 1 - 3) / +1)
\end{python}
but it also might be as simple as
\begin{python}
-49 / 901 / 4
\end{python}

We have our other grammars \texttt{URL\_GRAMMAR} and \texttt{CGI\_GRAMMAR} which we can just plug in:
\begin{python}
https://user:password@www.google.com:33/abc?def=9
%11%55_%6b+
\end{python}
The \emph{Fuzzing Book} times how long this fuzzer takes, comparing it to the \texttt{simple\_grammar\_fuzzer}
above, and finds that it's much faster and produces smaller inputs. It concludes that we have much better control
over grammar production.

And, yes, we work just fine with the EBNF grammar motivating all of this extra development.
\begin{python}
    expr_grammar = convert_ebnf_grammar(EXPR_EBNF_GRAMMAR)
    f = GrammarFuzzer(expr_grammar, max_nonterminals=10)
    print (f.fuzz())
\end{python}
yields, for instance, input
\begin{python}
97.01 * 5 * 7 - 8 * 9 - (9 + 0)
\end{python}
and not an infinite loop.

We will continue with techniques for learning grammars, probably.

\bibliographystyle{alpha}
\bibliography{L09}

\end{document}
